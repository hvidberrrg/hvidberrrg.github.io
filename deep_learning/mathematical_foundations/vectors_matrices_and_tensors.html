<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name='description' content='Basic introduction to linear algebra with a focus
        on vector, matrix, and tensor operations relavant for Deep Learning' />
        <meta name='keywords' content='Vectors, matrices, tensors, linear algebra' >
        <title>Vectors, matrices, and tensors</title>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-112452759-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag() {
                dataLayer.push(arguments);
            }
            gtag('js', new Date());

            gtag('config', 'UA-112452759-1');
        </script>
        <!-- MathJax -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
            TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
        <!-- Styling -->
        <link href="../../css/styling.css" rel="stylesheet">
    </head>
    <body>
        <div class="header">
            <ol class="breadcrumb" vocab="http://schema.org/" typeof="BreadcrumbList">
  				<li property="itemListElement" typeof="ListItem">
  					<a href="../../index.html" property="item" typeof="WebPage">
  						<span property="name">Hvidberrrg@GitHub</span>
  					</a>
  					<meta property="position" content="1">
  				</li>
  				<li property="itemListElement" typeof="ListItem">
  					<a href="../../deep_learning_and_neural_networks.html" property="item" typeof="WebPage">
  						<span property="name">Deep learning</span>
  					</a>
  					<meta property="position" content="2">
  				</li>
  				<li property="itemListElement" typeof="ListItem">
  					<a href="../mathematical_foundations_of_deep_learning.html" property="item" typeof="WebPage">
  						<span property="name">Mathematical Foundations</span>
  					</a>
  					<meta property="position" content="3">
  				</li>
  				<li property="itemListElement" typeof="ListItem">
  					<a href="#" property="item" typeof="WebPage">
  						<span property="name">Vectors, matrices, and tensors</span>
  					</a>
  					<meta property="position" content="4">
  				</li>
			</ol>
            <h1>Vectors, matrices, and tensors</h1>
        </div>
        <div class="content">
        	<div class="bodytext">
        	<p>This page gives a quick introduction to linear algebra with a dedicated focus 
        	on the vector and matrix operations used in relation with Deep Learning.
        	</p>
        	<h2 id="hed:vectors">Vectors</h2>
        	<p>Vectors are typically introduced as representations of quantities that have
        	direction and magnitude. For example, the velocity of a car is defined by its 
        	speed and the direction in which the car moves. This could be represented by a vector
        	whose direction, \(\theta\), is the same as that of the car, and whose length is 
        	proportional to the speed of the car. An example of such a vector is illustrated 
        	in the figure below.
        	</p>
        	<div class="figure" id="fig:a_vector">
            	<img src="assets/vector.png" alt="An example of a vector"/>
            	<span class="caption">Figure 1: A vector</span>
			</div>
			<p>The above figure also lends itself well to the introduction of some notation. 
			In the following we'll denote vectors in bold face letters, e.g. \(\mathbf{v}\), 
			while the <a href="#hed:vector_norm">magnitude</a> of a vector \(\mathbf{v}\) 
			will be denoted by \(\left\lVert\mathbf{v}\right\rVert\). \(\theta\) is the angle 
			between the vector and some reference direction.
			</p>
			<p>Vectors are also used to specify location and displacement in a mathematical space.
			An example is given in <a href="#fig:vector_components">figure 2</a> where the vector
			\(\mathbf{v}\)  represents a displacement in the plane (shown as a bold black arrow) 
			by \(v_1\) in the first axis and \(v_2\) in the second, i.e. \(\mathbf{v}\) can be seen
			as directed from the chosen origin to a point on the Cartesian plane with coordinates
			\((v_1, v_2)\). 
			</p>
			<div class="figure" id="fig:vector_components">
            	<img src="assets/vector_components.png" alt="Vector representing a displacement in 2-dimensional space"/>
            	<span class="caption">Figure 2: Vector representing a displacement in 2-dimensional space</span>
			</div>
			<p>Formally a vector is defined as an ordered set of \(n\) numbers that is usually 
			written as a vertical array, surrounded by square brackets:
			
			$$\begin{equation}
            \label{eq:vector_definition}
			\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}
			\end{equation}$$
			
			This object is called a <em>column vector</em> while the values in the array 
			are called the <em>elements</em> of the vector. The <em>size</em> (also referred 
			to as the <em>order</em>, <em>dimension</em> or <em>length</em>) of the vector is the number of elements 
			it contains. The vector above, for example, is of size \(n\) and is called an 
			<em>\(n\)-vector</em>.
			</p>
			
			<p>
			A horizontal array containing \(n\) numbers is called a <em>row vector</em>, e.g. 
			
			$$\begin{equation}
            \label{eq:row_vector_definition}
			\mathbf{u} = \begin{bmatrix} u_1 & u_2 & \cdots & u_n \end{bmatrix}
			\end{equation}$$
			
			If the term <em>vector</em> is used without being qualified further, it is understood
			to be a <em>column vector</em> as defined by \eqref{eq:vector_definition}.
			</p>
			
			
			<h3 id="hed:vector_transposition">Vector Transposition</h3>
			<p>Generally speaking, transposition converts row vectors into column vectors and vice versa.
			A given vector and its transpose form both contains the same elements, in the same order - 
			only the "orientation" of the vector is different.</p>
			
			<p>The <em>transpose</em> of a vector \(\mathbf{v}\) is denoted \(\mathbf{v}^\top\). Consequently 
			the transpose of the column vector defined in \eqref{eq:vector_definition} is the row vector:
			
			$$\begin{equation}
            \label{eq:transpose_vector}
			\mathbf{v^\top} = \begin{bmatrix} v_1 & v_2 & \cdots & v_n \end{bmatrix}
			\end{equation}$$
			
			while the transpose of the row vector, \(\mathbf{u}\), given by \eqref{eq:row_vector_definition} 
			is the column vector:
			
			$$\begin{equation}
            \label{eq:transpose_row_vector}
			\mathbf{u^\top} = \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix}
			\end{equation}$$
			
			If we transpose a vector twice, we get back the original vector, i.e.
			
			$$\begin{equation}
            \label{eq:double_vector_transpose}
			\mathbf{(v^\top)^\top} = \mathbf{v}
			\end{equation}$$
			</p>
			
			<h4 id="vector_transposition_example">Example</h4>
			<p>
			As an example, the transpose of \(\mathbf{v} = \begin{bmatrix} 1 \\ -2 \\ 3 \end{bmatrix}\)
			is \(\mathbf{v^\top} = \begin{bmatrix} 1 & -2 & 3 \end{bmatrix}\) and it's seen that transposing
			\(\mathbf{v^\top}\) gives us back \(\mathbf{v}\).
			</p>
			
			
			<h3 id="hed:inner_product">Inner product</h3>
			<p>The <em>inner product</em>, also referred to as the <em>dot product</em> of two column 
			vectors \(\mathbf{u}\) and \(\mathbf{v}\), of same order \(n\), is the scalar function:
			
			<span class="marginnote"><br/><br/><br/>\(\sum\) (Greek capital sigma) denotes 
			<a href="https://en.wikipedia.org/wiki/Summation#Capital-sigma_notation" target="_blank">summation</a>.</span>
			$$\begin{equation}
            \label{eq:inner_product}
			\mathbf{u} \cdot \mathbf{v} = \sum\limits_{i=1}^n u_i v_i = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n
			\end{equation}$$
			
			In other words: the dot product of two column vectors is the sum of the pairwise products 
			of the elements of the two vectors.
			</p>

			<p>
			Combined with the discussion on <a href="#hed:vector_transposition">vector transposition</a> we
			see that the inner product can be written using the following notation:
			
			$$\begin{equation}
            \label{eq:inner_product_transpose_notation}
			\mathbf{u} \cdot \mathbf{v} = \mathbf{u}^\top \mathbf{v} = \mathbf{v}^\top \mathbf{u}
			\end{equation}$$
            </p>
            
            <h4 id="hed:inner_product">Example</h4>
            <p>
            The dot product of of the two 3-vectors \(\mathbf{u} = \begin{bmatrix} 1 \\ -2 \\ 3 \end{bmatrix}\)
            and \(\mathbf{v} = \begin{bmatrix} 7 \\ -2 \\ -4 \end{bmatrix}\) is

            $$\begin{align}
            \mathbf{u} \cdot \mathbf{v} &= \mathbf{u}^\top \mathbf{v} \notag \\
            &= \begin{bmatrix} 1 & -2 & 3 \end{bmatrix} \begin{bmatrix} 7 \\ -2 \\ -4 \end{bmatrix} \notag \\
            &= 1\times7 + (-2)\times(-2) + 3\times(-4) \notag \\
            &= 7 + 4 -12 \notag \\
            &= -1 \notag
            \end{align}$$
            </p>
            
            <h4 id="geometric_definition_of_dot_product">Geometric definition of the vector dot product</h4>
            <p>
            In <a href="https://en.wikipedia.org/wiki/Euclidean_space" target="_blank">Euclidean space</a> a vector 
            is an object that possesses both a direction and a magnitude. Such a vector, \(\mathbf{v}\), is typically 
            pictured as an arrow whose length represents the magnitude, \(\left\lVert\mathbf{v}\right\rVert\), of the
            vector. The direction of the vector is given by the direction in which the arrow points.
            </p>
            <p>
            Suppose two vectors \(\mathbf{u}\) and \(\mathbf{v}\) are separated by an angle \(\theta\). The 
            geometric definition of their <em>inner product</em> is defined as
             $$\begin{equation}
            \label{eq:inner_product_geometric_definition}
			\mathbf{u} \cdot \mathbf{v} = \left\lVert\mathbf{u}\right\rVert \left\lVert\mathbf{v}\right\rVert \cos(\theta)
			\end{equation}$$
			
			where \(\left\lVert\mathbf{u}\right\rVert\) and \(\left\lVert\mathbf{v}\right\rVert\) are the 
            magnitudes of \(\mathbf{u}\) and \(\mathbf{v}\) respectively.
            </p>
            <p>
            In other words, the dot product of two vectors are defined geometrically as the size of the scalar
            projection of one of the vectors onto the other, multiplied with the size of the other vector (the one
            that is projected onto), as illustrated in <a href="#fig:scalar_projection">figure 3</a>.
            </p>
            <div class="figure" id="fig:scalar_projection">
            	<img src="assets/scalar_projection.png" alt="Scalar projection of one vector onto another"/>
            	<span class="caption">Figure 3: Scalar projection of one vector onto another</span>
			</div>
			<p>
			Intuitively \eqref{eq:inner_product_geometric_definition} says something about how "well aligned"  
			two vectors are. If they are orthogonal, then \(\cos(\theta)\) is \(0\) and consequently their inner
			product is also \(0\). If the vectors are contradirectional - i.e. pointing in opposite directions -
			 \(\cos(\theta)\)  assumes the value \(-1\). At the other extreme,  \(\cos(\theta)\) assumes the value
			 \(1\) for codirectional vectors, i.e. vectors pointing in the same direction. 
			</p>
            <p>As a given vector is obviously codirectional with respect to itself, then \eqref{eq:inner_product_geometric_definition}
            implies that the inner product of a vector \(\mathbf{v}\) with itself is
            
             $$\begin{equation}
            \label{eq:inner_product_of_vector_with_itself}
			\mathbf{v} \cdot \mathbf{v} = \left\lVert\mathbf{v}\right\rVert ^2
			\end{equation}$$
			
			Which right away gives us the Euclidean length of \(\mathbf{v}\) as
			<span class="marginnote"><br/><br/>Note that the Pythagorean theorem applied to the vector in 
			<a href="#fig:vector_components">figure 2</a> gives the same result.</span>
			$$\begin{equation}
            \label{eq:euclidean_length_of_vector}
			\left\lVert\mathbf{v}\right\rVert = \sqrt{\mathbf{v} \cdot \mathbf{v}}
			\end{equation}$$
			
			The Euclidean length is just one way of measuring the "magnitude" of a vector, though.
			So in the next section we'll have a closer look at the general concept of the
			<a href="#hed:vector_norm">vector norm</a>
            </p>
            
            
			<h3 id="hed:vector_norm">Vector Norm</h3>
			<p>
			A mathematical norm is a generalization of the general concept of length or size,
			i.e. a norm is a function that assigns a positive magnitude or length to each
			vector in a vector space (except for the zero vector that's assigned a magnitude 
			of \(0\)). Formally we define a <em>vector norm</em> as a function
			\(\left\lVert \cdot \right\rVert \colon \mathbb{R}^{n} \to \mathbb{R}\)
			that has the following properties
			
			<span class="marginnote"><br/><br/><br/><br/>
			\(\left\lvert \alpha \right\rvert\) denotes the <em id="absolute_value_definition">absolute value</em> of \(\alpha\).
			For any \(\alpha \in \mathbb{R}\) the absolute value of \(\alpha\) is defined as
			\(\left\lvert \alpha \right\rvert = \begin{cases} \alpha \text{, if } \alpha \ge 0 \\
			-\alpha \text{, if } \alpha \lt 0
			\end{cases} \) <br/>
			e.g. \(\left\lvert 42 \right\rvert = 42\) and \(\left\lvert -42 \right\rvert = 42\).<br/>
			Alternatively the absolute value of \(\alpha\) can be defined as
			\(\left\lvert \alpha \right\rvert = \sqrt{\alpha ^2}\)
			</span>
			
			<ol>
			<li>\(\left\lVert \mathbf{v} \right\rVert \ge 0\) for any vector \(\mathbf{v} \in \mathbb{R}^{n}\),
			and \(\left\lVert \mathbf{v} \right\rVert = 0\) if and only if \(\mathbf{v} = 0\) </li>
			
			<li>\(\left\lVert \alpha \mathbf{v} \right\rVert = \left\lvert \alpha \right\rvert \left\lVert \mathbf{v} \right\rVert\)
			for any vector \(\mathbf{v} \in \mathbb{R}^{n}\) and any scalar \(\alpha \in \mathbb{R}\)</li>
			
			<li>\(\left\lVert \mathbf{u} + \mathbf{v} \right\rVert \le \left\lVert \mathbf{u} \right\rVert + \left\lVert \mathbf{v} \right\rVert\)
			for any vectors \(\mathbf{u}, \mathbf{v} \in \mathbb{R}^{n}\)
			</li>
			</ol>
			Where the last property is called the <em>triangle inequality</em>. To get an 
			intuitive understanding of the triangle inequality, consider the case where you 
			are adding two vectors in 2-dimensional space as illustrated in <a href="#fig:vector_addition">
			figure 4</a>. It should be quite obvious that the (Euclidean) length of the sum of 
			the two vectors are always less than or equal to the sum of the length of the individual
			vectors.
			
			<div class="figure" id="fig:vector_addition">
            	<img src="assets/vector_addition.png" alt="Illustrating the triangle inequality in 2-dimensional space"/>
            	<span class="caption">Figure 4: Illustrating the triangle inequality in 2-dimensional space</span>
			</div>
			
			</p>
			
			<p>Defining a norm for a vector space allows us to characterize any given vector by
			a single, positive scalar value, i.e. the norm gives us an easy way of comparing vectors
			in the same space. The "trouble" with norms, though, is that there are so many of them
			and we need to choose one that's appropriate in the sense that it represents the entities of
			the problem we are trying to solve, and that the norm is computable at a tolerable cost.
			</p>
			
			<h4 id="hed:p-norm">The <i>p</i>-norm</h4>
			<p>The class of \(p\)-norms is a generalization of the Euclidean length of a vector \eqref{eq:euclidean_length_of_vector}
			and has many applications in mathematics, physics, and computer science.
			 The \(p\)-norm is also known as the \(L^p\)-norm, and is for any real number \(p \ge 1\) defined by
			 
			$$\begin{equation}
            \label{eq:p-norm_definition}
			\left\lVert \mathbf{v} \right\rVert _p = 
			\left(\sum\limits_{i=1}^n \left\lvert v_i \right\rvert ^p \right) ^\frac{1}{p} 
			\end{equation}$$
			
			where \(\mathbf{v} \in \mathbb{R}^{n}\) and \(\left\lvert v_i \right\rvert\)
			is the <a href="#absolute_value_definition">absolute value</a>
			of the \(i\)th element of the vector \(\mathbf{v}\). It can indeed be shown
			that \(\left\lVert \cdot \right\rVert _p\) defines a vector norm, but it's 
			beyond the scope of these notes.
			</p>
			
			<h5 id="hed:L1-norm">The \(L^1\)-norm</h5>
			<p>Setting \(p=1\) gives us the <em>\(L^1\)-norm</em>
			
			$$\begin{equation}
            \label{eq:1-norm_definition}
			\left\lVert \mathbf{v} \right\rVert _1 = 
			\left\lvert v_1 \right\rvert + \left\lvert v_2 \right\rvert + \cdots + \left\lvert v_n \right\rvert 
			\end{equation}$$
			
			i.e. the \(L^1\)-norm for a given vector is the sum of the absolute values of all 
			elements of the vector. The \(L^1\)-norm is also known as the <a target="_blank"
			href="https://en.wikipedia.org/wiki/Taxicab_geometry"><em>taxicab norm</em></a> since an
			analogy is offered by taxies driving in a grid street plan (like e.g. that of 
			Manhattan) - such taxies should not measure distance in terms of the length of 
			the straight line between their starting point and destination, but in terms 
			of the rectilinear distance* 
			<span class="marginnote">* The <em>rectilinear distance</em> is defined as the 
			distance between two points measured along axes at right angles.</span>
			between the two points.
			</p>
			
			<h5 id="hed:L2-norm">The \(L^2\)-norm</h5>
			<p>For \(p=2\) we have the <em>\(L^2\)-norm</em> - the well-known <em>Euclidean 
			norm</em> that we saw in \eqref{eq:euclidean_length_of_vector}
			
			$$\begin{equation}
            \label{eq:2-norm_definition}
			\left\lVert \mathbf{v} \right\rVert _2 = 
			\sqrt{\sum\limits_{i=1}^n v_i^2} = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2} 
			\end{equation}$$
			
			Combining this with \eqref{eq:inner_product_transpose_notation} we notice that
			the \(L^2\)-norm for a vector \(\mathbf{v}\) can be expressed in terms of the vectors 
			inner product with itself
			
			$$\begin{equation}
            \label{eq:2-norm_expressed_by_inner_product}
			\left\lVert \mathbf{v} \right\rVert _2 =  \sqrt{\mathbf{v}^\top \mathbf{v}}
			\end{equation}$$
			
			The \(L^2\)-norm is probably the most commonly encountered vector norm, and is often
			referred to simply as <em>the</em> norm.
            </p>
            
 			<h5 id="hed:L-infinity-norm">The \(L^\infty\)-norm</h5>
 			<p>The special case where \(p = \infty\) is defined as
 			
 			$$\begin{equation}
            \label{eq:max-norm_definition}
			\left\lVert \mathbf{v} \right\rVert _\infty = \max_{1 \le i \le n} \left\lvert v_i \right\rvert
			\end{equation}$$
 			
 			i.e. the <em>\(L^\infty\)-norm</em> gives you the absolute value of the vector element
 			that has the largest absolute value of all the elements of the vector. Hence this 
 			vector norm is also known as the <em>max-norm</em>.
 			</p>
 			
 			<h4 id="vector_norm_example">Example</h4>
 			<p>The table below summarizes the vector norms discussed above (plus a couple more :-).
 			 Furthermore the values of the norms are given for the example row vector 
 			\(\mathbf{v} = \begin{bmatrix} 1 & 2 & 3 \end{bmatrix}\).
 			<table>
            	<thead>
            		<tr>
            			<th>Name</th>
            			<th>Symbol</th>
            			<th>Value</th>
            		</tr>
            	</thead>
            	<tbody>
            		<tr>
            			<td>\(L^1\)-norm</td>
            			<td>\(\left\lVert \mathbf{v} \right\rVert _1\)</td>
            			<td>6</td>
            		</tr>
            		<tr>
            			<td>\(L^2\)-norm</td>
            			<td>\(\left\lVert \mathbf{v} \right\rVert _2\)</td>
            			<td>\(\sqrt{1^2 + 2^2 + 3^2} \sim 3.742\)</td>
            		</tr>
            		<tr>
            			<td>\(L^3\)-norm</td>
            			<td>\(\left\lVert \mathbf{v} \right\rVert _3\)</td>
            			<td>\(\left(1^3 + 2^3 + 3^3\right)^\frac{1}{3} \sim 3.302\)</td>
            		</tr>
            		<tr>
            			<td>\(L^4\)-norm</td>
            			<td>\(\left\lVert \mathbf{v} \right\rVert _4\)</td>
            			<td>\(\left(1^4 + 2^4 + 3^4\right)^\frac{1}{4} \sim 3.146\)</td>
            		</tr>
            		<tr>
            			<td>\(L^5\)-norm</td>
            			<td>\(\left\lVert \mathbf{v} \right\rVert _5\)</td>
            			<td>\(\left(1^5 + 2^5 + 3^5\right)^\frac{1}{5} \sim 3.077\)</td>
            		</tr>
            		<tr>
            			<td>\(L^\infty\)-norm</td>
            			<td>\(\left\lVert \mathbf{v} \right\rVert _\infty\)</td>
            			<td>3</td>
            		</tr>
            	</tbody>
            </table>
            From these examples it is also evident that the values of
            \(\left\lVert \cdot \right\rVert _p\) converges fairly quickly to those of
            \(\left\lVert \cdot \right\rVert _\infty\) as the value of \(p\) grows.
            </p>

			<p>
<!-- 
<br/>
\(v_1\)
<br/>
\(v_2\)
<br/>
\(\mathbf{v}\)
<br/>
\(\left\lVert\mathbf{v}\right\rVert\)
<br/>
\(\theta\)
<br/>
\(\mathbf{u}\)
<br/>
\(\mathbf{u} + \mathbf{v}\)
<br/>
\(\left\lVert\mathbf{u}\right\rVert\)
<br/>
\(\left\lVert\mathbf{u}\right\rVert\left\lVert\mathbf{v}\right\rVert\cos(\theta)\)
<br/>
\(\left\lVert\mathbf{u}\right\rVert\cos(\theta)\)
-->
			</p>
			
			<h2 id="hed:references">References</h2>
			<p>
			<ul class="bibliography">
        		<li id="Boyd2018">[Boyd2018]&nbsp;&nbsp;&nbsp;Stephen Boyd &amp; Lieven Vandenberghe,
        		<a href="https://web.stanford.edu/~boyd/vmls/vmls.pdf" target="_blank"><cite>Introduction to Applied Linear Algebra - 
        		Vectors, Matrices, and Least Squares</cite></a>,
        		Cambridge University Press, <time datetime='2018'>2018</time>
        		</li>
        		
        		<li id="Lambers2010">[Lambers2010]&nbsp;&nbsp;&nbsp;Jim Lambers,
        		<a href="http://www.math.usm.edu/lambers/mat610/sum10/lecture2.pdf" target="_blank">
        		<cite>Lecture 2 Notes (on Numerical Linear Algebra)</cite></a>,
        		The University of Southern Mississippi, <time datetime='2010'>2010</time>
        		</li>
			</ul>
	 
			</p>
			
        	</div>
        	<div class="margin">
        	<p>
        		<a class="margin" href="/deep_learning/activation_functions_in_artificial_neural_networks.html">Activation&nbspFunctions</a>
        	</p>
        	<p>
        		<a class="margin" href="/deep_learning/mathematical_foundations_of_deep_learning.html">Mathematical&nbspFoundations</a>
        	</p>
        	<p>
        		<a class="margin" href="/deep_learning/resources_and_references.html">Resources&nbsp&amp;&nbspreferences</a>
        	</p>
        	</div>
        </div>
    </body>
</html>
